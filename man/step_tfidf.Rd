% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tfidf.R
\name{step_tfidf}
\alias{step_tfidf}
\alias{tidy.step_tfidf}
\title{Term frequency-inverse document frequency of tokens}
\usage{
step_tfidf(recipe, ..., role = NA, trained = FALSE, columns = NULL,
  tf_weight = "raw count", K = 0.5, idf_weight = "idf",
  idf_adjustment = 1, res = NULL, prefix = "tfidf", skip = FALSE)

\method{tidy}{step_tfidf}(x, ...)
}
\arguments{
\item{recipe}{A recipe object. The step will be added to the
sequence of operations for this recipe.}

\item{...}{One or more selector functions to choose variables.
For `step_tfidf`, this indicates the variables to be encoded
into a list column. See [recipes::selections()] for more
details. For the `tidy` method, these are not currently used.}

\item{role}{Not used by this step since no new variables are
created.}

\item{trained}{A logical to indicate if the recipe has been
baked.}

\item{columns}{A list of tibble results that define the
encoding. This is `NULL` until the step is trained by
[recipes::prep.recipe()].}

\item{tf_weight}{A character determining the weighting scheme for
the term frequency calculations. Must be one of "binary", 
"raw count", "term frequency", "log normalization" or
"double normalization". Defaults to "raw count".}

\item{K}{A numeric weight used if `tf_weight` is set to
"double normalization". Defaults to 0.5.}

\item{idf_weight}{A character determining the weighting scheme
for the inverse document frequency calculations. Must be one of
"unary", "idf", "idf smooth" or "idf max".
Defaults to "idf".}

\item{idf_adjustment}{Numeric added to the denominator of inverse
document frequency calculation to avoid division-by-zero. Defaults
to 1.}

\item{res}{The words that will be used to calculate the term 
frequency will be stored here once this preprocessing step has 
be trained by [prep.recipe()].}

\item{prefix}{A character string that will be the prefix to the
resulting new variables. See notes below}

\item{skip}{A logical. Should the step be skipped when the
recipe is baked by [recipes::bake.recipe()]? While all
operations are baked when [recipes::prep.recipe()] is run, some
operations may not be able to be conducted on new data (e.g.
processing the outcome variable(s)). Care should be taken when
using `skip = TRUE` as it may affect the computations for
subsequent operations}

\item{x}{A `step_tfidf` object.}
}
\value{
An updated version of `recipe` with the new step added
 to the sequence of existing steps (if any).
}
\description{
`step_tfidf` creates a *specification* of a recipe step that
 will convert a list of tokens into multiple variables containing
 the Term frequency-inverse document frequency of tokens.
}
\details{
Term frequency-inverse document frequency is the product of two statistics.
The term frequency (TF) and the inverse document frequency (IDF). 

Term frequency is a weight of how many times each token appear in each 
observation. There are different ways to calculate the weight and this 
step can do it in a couple of ways. Setting the argument `tf_weight` to
"binary" will result in a set of binary variables denoting if a token
is present in the observation. "raw count" will count the times a token
is present in the observation. "term frequency" will devide the count
with the total number of words in the document to limit the effect 
of the document length as longer documents tends to have the word present
more times but not necessarily at a higher procentage. "log normalization"
takes the log of 1 plus the count, adding 1 is done to avoid taking log of
0. Finally "double normalization" is the raw frequency divided by the raw 
frequency of the most occurring term in the document. This is then 
multiplied by `K` and `K` is added tot he result. This is again done to 
prevent a bias towards longer documents.

Inverse document frequency is a measure of how much information a word
gives, in other words, how common or rare is the word across all the 
observations. If a word appears in all the observations it might not
give us that much insight, but if it only appear in some it might help
us differentiate the observations. 

Setting the argument `idf_weight` to "unary" will result in a IDF of 1,
thus simplifying this step to `step_tf`. "idf" is calculated by deviding
the number of observations with how many observatiuons the token appear 
in and taking the log of that. This will result in a devide-by-zero error
if a token doesn't appear in the data so an adjustment is done by adding 
`idf_adjustment` to the count of appearences. "idf smooth" is done by 
taking the log of 1 plus "idf". "idf max" is done in a similar way to
"idf" but instead of looking at the number of observations, we look at the
maximum number of observations a term appeared.

The new components will have names that begin with `prefix`, then
the name of the variable, followed by the tokens all seperated by
`-`. The new variables will be created alphabetically according to
token.
}
\examples{
library(recipes)

data(okc_text)

okc_rec <- recipe(~ ., data = okc_text) \%>\%
  step_tokenize(essay0) \%>\%
  step_tfidf(essay0)
  
okc_obj <- okc_rec \%>\%
  prep(training = okc_text, retain = TRUE)
  
bake(okc_obj, okc_text)

tidy(okc_rec, number = 2)
tidy(okc_obj, number = 2)
}
\seealso{
[step_hashing()] [step_tf()] [step_tokenize()]
}
